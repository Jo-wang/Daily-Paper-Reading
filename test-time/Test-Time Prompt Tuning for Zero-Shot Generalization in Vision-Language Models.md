## [Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models](https://arxiv.org/pdf/2209.07511.pdf)

NeurIPS2022

### Introduction and background
- The motivation of this paper is to address the challenge of zero-shot generalization in vision-language models. 
- While pre-trained models have shown impressive performance on in-domain tasks, they often struggle to generalize to unseen domains and categories. 
- The authors aim to improve the generalization capability of vision-language models by introducing test-time prompt tuning (TPT). 
- TPT allows the model to dynamically adapt its prompts during inference, enabling it to handle unseen data better and achieve higher accuracy in zero-shot scenarios. 
- The motivation is to explore the potential of prompt tuning as a means to enhance the robustness and generalization of large-scale foundation models in machine learning systems.

### Method
<img width=600 alt="aa" src="https://github.com/Jo-wang/Daily-Paper-Reading/assets/46414159/da54af86-520b-47c3-a5a6-48cabf069f5e">

### Experiments

### Notes
