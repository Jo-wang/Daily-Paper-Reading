## [Delving Deep into the Generalization of Vision Transformers under Distribution Shifts](https://arxiv.org/abs/2106.07617)

CVPR2022

Ranking: ⭐ ⭐ ⭐ ⭐

### Introduction and background
- transformer has made remarkable achievements
- the generalization ability of Vision Transformers (ViTs) is still less understood
- out-of-distribution (OOD) generalization is a highly desirable capability of machine learning models
- Recent works indicate current CNN architectures generalize poorly on various distribution shifts (DS)
- taxonomy of DS into four conceptual groups: background shifts, corruption shifts, texture shifts, and style shifts.

- Some conclusions:
  - ViTs learn weaker biases on backgrounds and textures, but stronger inductive biases towards shapes and structures, which is more consistent with human cognitive traits. So we can think ViT has better generalization than CNNs.
  - Large scale ⬆️ OOD generalization ⬆️
### Method

### Experiments

### Notes
