
## [Mitigating Neural Network Overconfidence with Logit Normalization](https://proceedings.mlr.press/v162/wei22d/wei22d.pdf)

ICML2022

### Introduction and background
- neural networks are known to suffer from the overconfidence issue, where they produce abnormally high confidence for both in- and out-of-distribution inputs.
- the norm of the logit keeps increasing during training, leading to overconfident output. 
- xxx

### Method

### Experiments

### Notes
